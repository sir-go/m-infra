---
stages:
  - cluster
  - k8s-init
  - cert-manager
  - ingress
  - charts-package
  - deploy
  - monitoring

variables:
  VERSION: 1.0.${CI_PIPELINE_ID}
  ALPINE_REPO: https://dl-cdn.alpinelinux.org/alpine/edge/testing/
  CM_URL: https://github.com/cert-manager/cert-manager/releases/download/v1.9.1/

.kube: &kube
  image:
    name: alpine:3.16.2
  before_script:
    - apk add kubectl helm curl jq --repository=${ALPINE_REPO}
    - mkdir ~/.kube && cp .kubeconfig ~/.kube/config
    - kubectl config use-context default

cluster:
  stage: cluster
  image:
    name: hashicorp/terraform:light
  variables:
    YC_BUCKET_NAME: m-tf
    YC_BUCKET_KEY: .tfstate
  before_script:
    - apk add jq
    - cd terraform && rm -rf .terraform*
    - echo ${YC_SERVICE_ACC_KEY_B64}|base64 -d > yc-key.json
    - >
      echo 'provider_installation { network_mirror {
          url = "https://terraform-mirror.yandexcloud.net/"
          include = ["registry.terraform.io/*/*"] }
          direct { exclude = ["registry.terraform.io/*/*"] }
      }' > ~/.terraformrc
  script:
    - |
      export TF_VAR_yc_folder_id=${YC_FOLDER_ID}
      export TF_VAR_yc_cloud_id=${YC_CLOUD_ID}
      export TF_VAR_yc_srv_acc_id=${YC_SERVICE_ACC_ID}
    - >
      terraform init
      -backend-config="bucket=${YC_BUCKET_NAME}"
      -backend-config="key=${YC_BUCKET_KEY}"
      -backend-config="access_key=${YC_BUCKET_KEY_ID}"
      -backend-config="secret_key=${YC_BUCKET_KEY_VALUE}"
    - terraform validate
    - terraform apply -auto-approve
    - terraform output -json|jq -r '."cluster-id".value' > ../cluster-id.txt
  artifacts:
    paths:
      - cluster-id.txt

k8s-init:
  stage: k8s-init
  image:
    name: alpine:3.16.2
  dependencies:
    - cluster
  variables:
    GIT_STRATEGY: none
    SERVICE_ACC_NAME: m-admin
  before_script:
    - apk add bash kubectl curl jq --repository=${ALPINE_REPO}
    - curl -sSL https://storage.yandexcloud.net/yandexcloud-yc/install.sh | bash
    - ln -s /root/yandex-cloud/bin/yc /usr/bin/
    - echo ${YC_SERVICE_ACC_KEY_B64}|base64 -d > yc-key.json
    - yc config profile create ${SERVICE_ACC_NAME}
    - yc config set service-account-key yc-key.json
    - export CLUSTER_ID=$(cat cluster-id.txt)
    - yc managed-kubernetes cluster get-credentials ${CLUSTER_ID} --external
  script:
    - |
      echo "apiVersion: v1
      kind: ServiceAccount
      metadata:
        name: ${SERVICE_ACC_NAME}
        namespace: kube-system
      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      metadata:
        name: ${SERVICE_ACC_NAME}
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: ClusterRole
        name: cluster-admin
      subjects:
      - kind: ServiceAccount
        name: ${SERVICE_ACC_NAME}
        namespace: kube-system" | kubectl apply -f -
    - >
      yc managed-kubernetes cluster get --id ${CLUSTER_ID} --format json |
      jq -r .master.master_auth.cluster_ca_certificate | awk '{gsub(/\\n/,"\n")}1' > ca.pem
    - >
      export SA_TOKEN=$(kubectl -n kube-system get secret $(kubectl -n kube-system get secret |
      grep ${SERVICE_ACC_NAME} | awk '{print $1}') -o json | jq -r .data.token | base64 -d)
    - >
      export MASTER_ENDPOINT=$(yc managed-kubernetes cluster get --id ${CLUSTER_ID} --format json |
      jq -r .master.endpoints.external_v4_endpoint)
    - >
      kubectl config set-cluster cluster
      --certificate-authority=ca.pem
      --embed-certs=true
      --server=${MASTER_ENDPOINT}
      --kubeconfig=.kubeconfig
    - >
      kubectl config set-credentials ${SERVICE_ACC_NAME} --token=${SA_TOKEN} 
      --kubeconfig=.kubeconfig
    - >
      kubectl config set-context default --cluster=cluster --user=${SERVICE_ACC_NAME} 
      --kubeconfig=.kubeconfig
    - chmod go-r .kubeconfig
  artifacts:
    paths:
      - .kubeconfig

cert-manager:
  stage: cert-manager
  variables:
    GIT_STRATEGY: none
  dependencies:
    - k8s-init
  <<: *kube
  script:
    - kubectl apply -f ${CM_URL}/cert-manager.crds.yaml
    - kubectl apply -f ${CM_URL}/cert-manager.yaml
    - |
      echo "apiVersion: cert-manager.io/v1
      kind: ClusterIssuer
      metadata:
        name: letsencrypt
        namespace: cert-manager
      spec:
        acme:
          server: https://acme-v02.api.letsencrypt.org/directory
          privateKeySecretRef:
            name: letsencrypt
          solvers:
            - http01:
                ingress:
                  class: nginx
      " | kubectl apply -f -

ingress:
  stage: ingress
  dependencies:
    - k8s-init
  <<: *kube
  script:
    - helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
    - helm repo update
    - >
      helm upgrade --install --atomic  ingress-nginx ingress-nginx/ingress-nginx
      --namespace default
    - >
      export EXT_IP=$(kubectl get svc ingress-nginx-controller 
      --namespace default -o json | 
      jq -r '.status.loadBalancer.ingress[0].ip')
    - echo "external IP ${EXT_IP}"
    - curl -q "${DNS_UPD_URL}?hostname=${FQDN}&myip=${EXT_IP}&password=${DYNDNS_PASSWORD}"

charts-package:
  stage: charts-package
  only:
    changes: [ helm/store/**/* ]
  image:
    name: alpine:3.16.2
  before_script:
    - apk add helm curl --repository=${ALPINE_REPO}
  script:
    - cd helm/store
    - >
      sed -Ei "s/^version:\s+([0-9]+\.){2}[0-9]+/version: ${VERSION}/" charts/backend/Chart.yaml
    - >
      sed -Ei "s/^version:\s+([0-9]+\.){2}[0-9]+/version: ${VERSION}/" charts/frontend/Chart.yaml
    - >
      sed -Ei "s/^appVersion:\s+.*$/appVersion: ${VER_BACK}/" charts/backend/Chart.yaml
    - >
      sed -Ei "s/^appVersion:\s+.*$/appVersion: ${VER_FRONT}/" charts/frontend/Chart.yaml
    - >
      sed -Ei "s/^version:\s+([0-9]+\.){2}[0-9]+/version: ${VERSION}/" Chart.yaml
    - >
      sed -Ei "s/backend,\s+version:\s+([0-9]+\.){2}[0-9]+/backend, version: ${VERSION}/" Chart.yaml
    - >
      sed -Ei "s/frontend,\s+version:\s+([0-9]+\.){2}[0-9]+/frontend, version: ${VERSION}/" Chart.yaml
    - helm package .
    - curl -u ${NEXUS_USERNAME}:${NEXUS_PASSWORD} ${NEXUS_HELM_REPO} --upload-file store-*.tgz

deploy:
  stage: deploy
  variables:
    GIT_STRATEGY: none
  only:
    changes: [ helm/store/**/* ]
  dependencies:
    - k8s-init
  <<: *kube
  script:
    - >
      helm repo add nexus ${NEXUS_HELM_REPO}
      --username ${NEXUS_USERNAME}
      --password ${NEXUS_PASSWORD}
    - helm repo update
    - >
      helm upgrade --install --atomic store
      --namespace default
      --set backend.image="backend:${VER_BACK}"
      --set frontend.image="frontend:${VER_FRONT}"
      --set global.environment=test
      --set global.regcreds="${REG_CREDS_B64}"
      --set frontend.fqdn="${FQDN}"
      nexus/store
      --version ${VERSION}

monitoring:
  stage: monitoring
  only:
    changes: [ helm/monitoring/* ]
  dependencies:
    - k8s-init
  <<: *kube
  script:
    - cd helm/monitoring
    - helm dependency build
    - |
      echo "
      - job_name: managed-kubernetes
        metrics_path: '/monitoring/v2/prometheusMetrics'
        params:
          folderId: [ ${YC_FOLDER_ID} ]
          service:
            - managed-kubernetes
        scrape_interval: 20s
        bearer_token: ${YC_BEARER}
        static_configs:
          - targets: ['monitoring.api.cloud.yandex.net']
            labels:
              folderId: ${YC_FOLDER_ID}
              service: ${YC_K8S_SERVICE_ID}
      " > scrap-conf.yaml
    - >
      helm upgrade --install --atomic --namespace default
      --set-file prometheus.extraScrapeConfigs=scrap-conf.yaml
      --set "prometheus.server.ingress.hosts[0]=prometheus.${FQDN}"
      --set "prometheus.server.ingress.tls[0].secretName=v-prometheus-tls" 
      --set "prometheus.server.ingress.tls[0].hosts[0]=prometheus.${FQDN}"
      --set "grafana.adminPassword=${GRAFANA_PASSWORD}"
      --set "grafana.ingress.hosts[0]=grafana.${FQDN}"
      --set "grafana.ingress.tls[0].secretName=v-grafana-tls" 
      --set "grafana.ingress.tls[0].hosts[0]=grafana.${FQDN}"
      monitoring .
